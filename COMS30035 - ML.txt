#separator:tab
#html:true
<div>What is supervised learning?</div><div></div>	A type of machine learning where the model learns from data that is labelled. Each input is paired with a correct output or 'target'.
<div>What is unsupervised learning?</div><div></div>	A type of machine learning where the model learns from unlabelled data, trying to find patterns or structure on its own.
<div>What is reinforcement learning?</div><div></div>	A type of machine learning where a model (or 'agent') learns by performing actions in an environment and receiving rewards or punishments.
<div>Define overfitting</div><div></div>	When a model is too complex and learns the training data too well, including its noise and random fluctuations. This makes it perform poorly on new, unseen data.
<div><div><div>Define underfitting</div><div></div></div><div></div></div><div></div>	When a model is too simple to capture the underlying patterns in the data, leading to poor performance on both training and new data.
<div>What is the purpose of model selection?</div><div></div>	To choose the best model for a given task by finding a balance that avoids both overfitting and underfitting, allowing the model to generalise well to new data.
<div>What are the roles of the training, validation, and testing datasets?</div><div></div>	<div>Training set: Used to train and optimise the model's parameters. <sup></sup><sup></sup><sup></sup><sup></sup><div></div><div></div></div><div></div><div>Validation set: Used to tune the model and select the best one during training. <sup></sup><sup></sup><sup></sup><sup></sup><div></div><div></div></div><div></div><div>Testing set: Used only after training is complete to provide an unbiased evaluation of the final model's performance on unseen data.&nbsp;</div>
<div>What is the curse of dimensionality?</div><div></div>	The problem where, as the number of features (dimensions) in the data increases, the amount of data needed to effectively cover the space grows exponentially.
"<div>What is the ""no free lunch"" theorem in machine learning?</div><div></div>"	The principle that there is no single machine learning model that is universally the best for every problem. The choice of model depends on the specific task and data.
<div>What is the difference between parametric and non-parametric models?</div><div></div>	Parametric models have a fixed number of parameters, regardless of the amount of training data.&nbsp;Non-parametric models have a number of parameters that can grow as the amount of training data increases.&nbsp;Parametric models are often simpler, faster to fit, and may require less data. Non-parametric models make less assumptions, usually produce better models, but are more expensive to train.
<div>What is the goal of a regression task?</div><div></div>	To find the relationship between variables in order to predict a continuous numerical output.
What is the formula for the least squares solution for linear regression parameters, \(\hat{\theta}\)?	\(\hat{\theta}=(X^{T}X)^{-1}X^{T}y\)&nbsp;This formula finds the parameters that minimise the sum of the squared differences between the predicted and actual values.
What is Maximum Likelihood Estimation (MLE)?	A method for estimating the parameters of a probabilistic model by finding the parameter values that maximise the likelihood of the observed data having occurred.
What does it mean for a dataset to be linearly separable?	The data points from different classes can be completely separated by a single straight line (or a plane/hyperplane in higher dimensions).
What type of function does logistic regression use to model the probability of a class?	The logistic sigmoid function, which produces an 'S'-shaped curve that maps any real-valued number to a value between 0 and 1.
What is gradient descent?	An optimisation algorithm used to find the minimum of a function. It works by iteratively taking steps in the opposite direction of the function's gradient (the direction of steepest descent).
What is cross-validation?	The original data is split into&nbsp;\(S\)&nbsp;folds. The procedure cycles through these folds, using&nbsp;\((S-1)/S\)&nbsp;of the data for training and the remaining&nbsp;\(1/S\)&nbsp;for validation in each run. The performance is typically computed as the average error across all&nbsp;\(S\)&nbsp;folds. This method is a solution to the problem that simply splitting the data leaves less data available for training, and it helps avoid tuning on a single, potentially small, validation set.
How can linear regression be extended to handle complexity beyond simple linear relationships with the original data&nbsp;\(x\)?	Linear regression can be applied to a feature vector&nbsp;\(\phi(x)\)&nbsp;rather than the original data&nbsp;\(x\), using the model:&nbsp;\[p(y|\phi(x), w) = w^\top \phi(x) + \epsilon\]<br>
What is the relationship between the least-squares solution and the Maximum Likelihood Estimation (MLE) solution in linear regression?	The least-squares solution is also the maximum likelihood solution. This is due to the assumption that the error&nbsp;\(\epsilon\)&nbsp;is Gaussian distributed.
For an input&nbsp;\(x\)&nbsp;and two classes,&nbsp;\(C_1\)&nbsp;and&nbsp;\(C_2\), how does Linear Discriminant assign a class label?	Linear Discriminant computes&nbsp;\(y = w^\top x\). It assigns&nbsp;\(x\)&nbsp;to class&nbsp;\(C_1\)&nbsp;if&nbsp;\(y \ge 0\), and assigns&nbsp;\(x\)&nbsp;to class&nbsp;\(C_2\)&nbsp;otherwise.
What two key assumptions are made about the data distribution to enable the parameters of a Linear Discriminant model to be learnt?	1) Data for each class must have a Gaussian distribution. <br>2) These two Gaussian distributions must share the same covariance matrix.
How are the parameters for Linear Discriminant found under the necessary distributional assumptions?	The parameters are found by applying Maximum Likelihood Estimation (MLE).
What is the definition of the logistic sigmoid function,&nbsp;\(\sigma(a)\).	\[\sigma(a) = \frac{1}{1 + \exp(-a)}\]<br>
What is the logistic regression model used for two-class classification?	The logistic regression model uses the sigmoid function to define the posterior probabilities for class&nbsp;\(C_1\)&nbsp;and&nbsp;\(C_2\): \(p(C_1|x) = \sigma(w^\top x)\),&nbsp;\(p(C_2|x) = 1 - p(C_1|x)\).
How are the Maximum Likelihood Estimation (MLE) parameters for logistic regression found?	By using gradient descent or by using the closed-form formula for the MLE parameters.
What are neural network weights?	These are the parameters of a neural network. A unit&nbsp;\(j\)&nbsp;computes a value&nbsp;\(a_j\)&nbsp;which is defined as a weighted sum of its inputs from the previous layer, utilising these weights (\(w_{ji}\)). The learning process involves iteratively updating these weights (parameters) by descending the gradient to minimise the error.
What is a neural network activation function?	A function applied by a unit&nbsp;\(j\)&nbsp;in a neural network (excluding the input layer). After computing the weighted sum of inputs (\(a_j\)), this sum is passed to a nonlinear activation function&nbsp;\(h\)&nbsp;to produce the unit's output value (\(z_j\)). These functions must be non-linear; otherwise, the entire neural network would simply represent a linear function, negating the benefit of using multiple layers (as you could just use linear or logistic regression).
What is the input layer of a neural network?	This is the initial layer of a neural network where the data is fed into the model.
What is a hidden layer in a neural network?	Any layer between the input and output layers.
What is the output layer of a neural network?	The final layer of a neural network that generates the model's prediction or outcome. For classification problems, an activation function like a sigmoid is typically used at the output layer.
What is a cost/loss function?	A function used to quantify the difference between the network's output and the target value (error). The goal of optimising the network is to minimize this function. In methods like weight decay, a regularisation term is added to this function to penalise large weights.
What is a forward pass?	The computational stage where an input vector is applied to the network, and activations are propagated through all units, layer by layer, from the input layer up to the output layer, resulting in the prediction. We need forward propagation to compute quantities used in backpropagation.
What is a backwards pass?	The stage following the forward pass where the error signal is propagated backwards across the network using the back propagation algorithm to compute the error gradient with respect to the weights. The computed gradient is then used to update the network parameters.
What is the vanishing gradient problem?	A phenomenon encountered when training deep neural networks where the gradient, during the backward pass, becomes very small. This prevents effective weight updates via gradient descent.
What is the exploding gradient problem?	A phenomenon encountered when training deep neural networks where the gradient becomes very large.
What is gradient clipping?	A method used to mitigate the exploding gradient problem by capping the magnitude of the gradient. The clipped gradient \(g'\)&nbsp;is calculated using the formula&nbsp;\(g' = \min \left( 1, \frac{c}{||g||} \right) g\), ensuring that&nbsp;\(g'\)&nbsp;points in the same direction as the original gradient&nbsp;\(g\)&nbsp;but with a restricted magnitude.
What are non-saturating activation functions?	Activation functions that do not converge on a constant value for extremely negative or positive input values.&nbsp;Standard sigmoid functions saturate at large inputs, causing their gradients to become zero, which contributes to the vanishing gradient problem. Choosing non-saturating functions, such as ReLU variants, addresses this issue.
What is a residual network?	A residual network is composed of residual layers. Each layer is defined by the function&nbsp;\(F'_l(x) = F_l(x) + x\), where&nbsp;\(F_l(x)\)&nbsp;represents the standard linear-activation-linear mapping. The primary benefit of this design is that it offers a 'short cut' which allows gradients to flow directly from the output layer to any previous layer, aiding in the training of very deep networks.
Why is parameter initialisation important?	The optimisation landscape for neural network training is non-convex (possessing many local minima/'valleys'), the starting position (the choice of initial weights) is important as standard gradient descent will converge to the local optimum corresponding to that starting point.
What is early stopping?	A method used to prevent overfitting in neural networks (and other models). A validation dataset is created, and training is stopped when the error on this validation set begins to increase, indicating that the model is starting to overfit the training data.
What is weight decay?	Also known as&nbsp;\(\ell_2\)&nbsp;regularisation, this method reduces overfitting by modifying the loss function. A penalty term,&nbsp;\(\lambda w^\top w\)&nbsp;(where&nbsp;\(w\)&nbsp;is the weight vector and&nbsp;\(\lambda &gt; 0\)&nbsp;is a user-chosen parameter), is added to the cost function to penalize large weights. This penalty term is the square of the 2-norm, or Euclidean norm, of the weight vector, \(||w||_2^2\).
What is dropout?	"A regularisation method for preventing overfitting in neural networks. During training, a unit's outgoing connections are randomly deactivated (removed) with a specified probability. This is done on a per-example basis—a new ""thinned network"" is sampled and trained for each presentation of a training case. This process forces units to become robust and prevents them from learning complex, fragile dependencies on specific other units."
How does an individual unit compute its output value?	A unit&nbsp;\(j\)&nbsp;first computes&nbsp;\(a_j\), a weighted sum of its inputs (\(z_i\)) from the previous layer, and then passes&nbsp;\(a_j\)&nbsp;to a nonlinear activation function&nbsp;\(h\)&nbsp;to determine its output&nbsp;\(z_j\):&nbsp;\(a_j = \sum_i w_{ji}z_i\), \(z_j = h(a_j)\).
What is the back propagation formula?	"\[\delta_j = h'(a_j) \sum_k w_{kj}\delta_k\]\(\mathbf{\delta_j}\): The error signal for unit&nbsp;\(j\).<br>\(a_j\)<span style=""white-space: pre;"">:</span>&nbsp;The total weighted sum of inputs received by unit&nbsp;\(j\)&nbsp;before the activation function is applied.<br>\(h'(a_j)\): Represents the derivative of the activation function,&nbsp;\(h\).<br>\(\sum_k\): Indicates the summation over all units&nbsp;\(k\)&nbsp;in the subsequent layer to which unit&nbsp;\(j\)&nbsp;sends connections.<br>\(w_{kj}\): The weight parameter connecting unit&nbsp;\(j\)&nbsp;to unit&nbsp;\(k\)&nbsp;in the next layer.<br>\(\delta_k\): The previously calculated error signal (\(\equiv \frac{\partial E_n}{\partial a_k}\)) for unit&nbsp;\(k\)&nbsp;in the layer subsequent to&nbsp;\(j\)."
What forms of learning require teaching signals? (Ie, to be told what is correct)	Supervised Learning (Targets/Labelled data) and Reinforcement Learning (Rewards)
<div><div>If logistic regression gives very similar results to the linear discriminant, why might we want to use it?</div></div>	Because it also gives us p(C|x),&nbsp;which can be&nbsp;very useful in practice. We know how uncertain the model is about its decision. (Think about the cat classifier. Linear discriminant would say it is or is not a cat. While regression says it is a cat with 0.86 certainty or something).
<div><div>What is the advantage of choosing differentiable activation functions?</div></div>	"Having differentiable activation functions allows us to compute the 
gradient of the error with respect to network weights thus allowing us 
to use gradient descent to fit weights to the data."
<div><div>Compared to other methods for regression or classification, decision trees have what advantages?</div></div>	<div><div>Their decisions are more interpretable than most other methods. (They tell you along which axis they are splitting the input space and by what threshold at each decision)</div></div><div><br></div><div><div><div>They divide the input space into regions that can be dealt with by expert models at the leaves of the tree. (From a quick convo with chat: Trees don't always just output a constant prediction, especially when they can't because its over a continous output space. Instead they act as sort of routers that direct an input to a specialised sub-model (or expert) that is trained on data only from that input region which then classifies it or makes a prediction.)</div></div></div>
<div><div>Decision trees can be learned using the CART algorithm, which grows the tree from by adding nodes one by one. At each point, it chooses the new node that minimises error by exhaustively searching over...</div></div>	input variables and threshold values...<br><br>But Jack, my beautiful, dashing and charming friend - how does it check every input variable when the inputs are continous?? Td;lr i don't think we have to worry about it, but it uses the input data as potential candidates for a split. Ie:<br><br>x_j:&nbsp; 1.2&nbsp;&nbsp; 2.5&nbsp;&nbsp; 2.7&nbsp;&nbsp; 3.1&nbsp;&nbsp; 3.9&nbsp;&nbsp; 5.4<br>y:&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp; &nbsp; &nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; 1&nbsp; &nbsp; &nbsp; 1&nbsp; &nbsp; &nbsp; 1&nbsp; &nbsp; &nbsp; 1<br><br>The thresholds CART would check look like this:<br>t∈{ (1.2+2.5)/2, (2.5+2.7)/2, (2.7+3.1)/2, (3.1+3.9)/2, (3.9+5.4)/2 }<br><br>So the midpoints. And then it just picks the one wich minimises whichever impurity you're using (like Gini) :D<br><br>
<div><div>The pruning criterion removes nodes from the tree that avoid increasing a pruning criterion. This criterion balances two things. What are they?</div></div>	"Residual error (fit to training data): how well the tree predicts the training set.<br><br>Number of leaf nodes (complexity): how large or complex the tree is.<br><br>Explanation if needed:<br><div>The balance between these two things is formalized using a cost-complexity criterion, such as in CART:</div>\(R_\alpha(T) = R(T) + \alpha |T|\)<br><br><div>where:</div>
<ul>
<li>
<div>R(T)&nbsp;= residual (training) error of tree T</div>
</li>
<li>
<div>∣T∣&nbsp;= number of leaf nodes in T</div>
</li>
<li>
<div>α&nbsp;= complexity penalty parameter</div></li></ul>
<div>Pruning then removes subtrees that don’t sufficiently reduce the overall cost \(R_\alpha(T)\). This prevents overfitting but too much pruning (too high a complexity parameter) means you'll underfit instead.</div><div><br></div><div>Even more indepth on pruning phase.</div><div><div>1) Start at the leaves (bottom of the tree).</div> <div>For each subtree t, we compare:</div><div><br></div>\(R_\alpha(t) = R(t) + \alpha |t|\)</div><div><br><div>with the cost of replacing that whole subtree t by a single leaf node:</div></div><div><br></div><div>\(R_\alpha(leaf(t)) = R(leaf(t)) + \alpha\)<br></div><div><br><div>2) If pruning that subtree decreases,&nbsp;\(R_\alpha\)&nbsp;we prune it — i.e. collapse it into a leaf.</div> <div>3) After pruning the bottom-most prunable subtrees, we move upward recursively, repeating the process for higher-level subtrees.</div></div>"
<div><div>Suppose you were applying a SVM to data with N datapoints&nbsp;where each datapoint&nbsp;has M features. How many support vectors would there be?</div></div>	Not possible to say.<br><br>The number of support vectors in an SVM depends on the data and margin — not directly on N (number of data points) or M (number of features).<br><br>But Jack!!! How&nbsp;<i>can</i>&nbsp;you tell when there'll be more or less support vectors...?<br><br><div>- Support vectors are the data points that lie on or within the margin boundaries and determine the optimal separating hyperplane.</div> <div>- In some cases, only a few points are support vectors (if the data is <b>easily</b> <b>separable</b>).</div> <div>- In others, many or even all of the points might become support vectors (especially if the data is <b>noisy</b> or the <b>margin is soft</b>. Soft margins allow for some misclassification or margin violations in order to achieve a better balance between accuracy and generalization while a hard margin&nbsp; allows for no misclassification).</div><br>
<div><div>When classifying a test datapoint&nbsp;using an SVM, is it necessary to access every training datapoint?</div></div>	No. Just the support vectors are needed!
<div><div>What is the name of the kernel function which just computes the scalar product of the original datapoints?</div></div>	A linear kernel. These are useful when your data is linearly seperable (or nearly). They are simpler and faster than nonlinear kernels like RBF or polynomial.<br><br>When are they useful?:<br><br><div>- In high dimensions (like text data, bag-of-words, or TF-IDF vectors), linear separation often works well because the data tends to be linearly separable due to the curse of dimensionality.</div> <div>Example:</div> <div>Gene expression data: often thousands of features and few samples — linear kernels work great.<br></div><div><br></div>- Linear SVMs are faster to train and the learned weights&nbsp;\(w\)&nbsp;<b>directly</b>&nbsp;show feature importance. Neat!<br><br>- Nonlinear kernels like RBF can overfit if the data is noisy or you have few samples. A linear kernel is a <strong>regularized</strong> and safer choice in such cases.<br>
<div><div>When using a RBF kernel what is the dimension of the implicit feature space?</div></div>	"<div><div>Infinite.</div></div><div><br></div><div>Fuck this btw like what.</div><div><br></div><div>Explanation (buckle up and be ready to forget cause idk if this matters):</div><div><br></div><div>So here's the RBF kernel formula as we know and use it:</div><div><br></div><div>\(K(\mathbf{x}_i, \mathbf{x}_j) = \exp\Big(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2\Big)\)</div><div></div><div><br></div><div>I'd show the proof for this version but it's disgusting, non-examinable and the point remains the same where we're using an infinite taylor series.</div><div><br></div><div>RBF (Guassian and statsy version) can also be written as:</div><div><br></div><div><div>For scalar inputs \(x, x'\)&nbsp;and bandwidth parameter \(σ&gt;0\),</div><div><br></div><div></div><div></div><div></div><div></div><div>\(k(x,x')=\exp\!\Big(-\frac{(x-x')^2}{2\sigma^2}\Big)\)<br></div></div><div><br></div><div>Which we can write as:</div><div><br></div><div>\(k(x, x') = \exp\!\left(-\frac{x^2}{2\sigma^2}\right)
           \exp\!\left(-\frac{{x'}^2}{2\sigma^2}\right)
           \exp\!\left(\frac{xx'}{\sigma^2}\right)\)<br></div><div><br></div><div>So now we do some taylor series bs...</div><div><br></div><div>\(\varphi_n(x) = \exp\!\left(-\frac{x^2}{2\sigma^2}\right)\frac{x^n}{\sigma^n \sqrt{n!}}\)&nbsp; &nbsp; &nbsp; &nbsp;\(n = 0, 1, 2, ...\)</div><div><br></div><div>So now we have x^n where n is just an infinite series of integers. So now we have an <i>implicit</i> feature space that is infinite-dimensional. Implicit because obv we don't use this explicitly, instead we use the kernel trick.</div><div><br></div><div>This is why RBF is extremely flexible — it can (in principle) represent arbitrarily complicated functions of the input. Don't get confused like I did though...<br></div><div><br></div><div><div>The RBF kernel defines an infinite set of possible directions (features). But the algorithm only ever combines the directions corresponding to the training data points. Hence, the model’s complexity is bounded by how many data points you have — not by the infinite capacity of the feature space. Also we use regularisation to keep complexity in check. We decide how much of that infinite complexity we actually use.</div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div>"
<div><div>If we wanted to guarantee that an SVM correctly classified every&nbsp;training data point what value should we set for the regularisation parameter C?</div></div>	<div><div>Infinity (in practice as big as possible).</div></div><div><br></div><div>Lets look at the formula to convicne ourselves of this:</div><div><br></div><div><div><br></div><div>A soft-margin SVM solves:</div><div><br>\(min_{w, b, \xi} \; \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{n} \xi_i\)</div></div><div><br></div><div>\(\text{subject to: } y_i (w^\top \phi(x_i) + b) \ge 1 - \xi_i, \quad \xi_i \ge 0\)</div><div><br></div><div>\(\xi_i\) is the slack variable for i-th training point. It's how much the point violated the margin. If the i-th point is correctly classified it's == 0. So if C = a big fuck off number (infinity) it will essentially enforce that \(\xi_i\) is 0 for all i. This turns the soft-margin into a hard-margin. :D</div><div><br></div><div><br></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div>
<div><div>When using an RBF kernel how do we force the influence of training data points to be local, so that a given training datapoint only significantly affects the classification of test data points which are close to it?</div></div>	<div>RBF kernel formula:</div><div><br></div><div>The value \(K(\mathbf{x}_i, \mathbf{x}_j)\)&nbsp;measures similarity between points. If \(xi\)​ and \(xj\) are close, K is near 1. If they are far apart, K approaches 0. When we make gamma rly big, the kernel decays faster with distance so the influence becomes more local.</div><div><br></div><div><div>Mathematically:</div>\(\text{If } \gamma \text{ is large, } \exp\Big(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2\Big) \approx 0 \text{ unless } \mathbf{x}_i \approx \mathbf{x}_j\)</div><div></div><div></div><div><br></div><div>Inversely, if gamma was 0, K = 1 for all values of xi and xj so every points influence is completely global and they're all perfectly similar.</div>
<div><div>How can SVM do classification when there are more than 2&nbsp;classes?</div></div>	One can learn SVMs&nbsp;for each pair of classes and combine the decisions these classifiers make to classify a given test datapoint. <br><br>For each class one can learn&nbsp;an&nbsp;SVM which distinguishes that class from all others, and then combine these classifiers to classify a new test datapoint.
<div><div>What is the factorisation&nbsp;of P(A,B,C) associated with the following directed acyclic graph (DAG):&nbsp;</div><div>A&nbsp;→ B&nbsp;← C&nbsp;?</div></div>	P(A)P(C)P(B|A,C).<br><br>Remember the arrows show the direction of conditionality and nodes with no parents are not conditional on anything.
<div><div>If A,B and C are all pairwise independent,&nbsp;which DAG represents this?</div></div>	<div><div>A B C.<br><br>Completely disconnected graph with no edges and three nodes.</div></div>
<div><div>Let A and B be binary random variables with values 0 or 1.</div><div>What is the probability distribution specified by the following DAG:&nbsp;A&nbsp;→ B ?</div></div>	"<div><div>It is not possible to say,&nbsp;a DAG on its own does not specify a particular probability distribution.</div></div><div><br></div><div>The question is essentially asking you ""What's the probability of A and B given that B is conditional on A?"", which doesn't make any sense.</div>"
"<div><div>What is the purpose of <b>plate notation</b>? Eg:</div><div><img src=""paste-8e41ed310fefa3d168dbaf93966af96bb19895a2.jpg""></div></div>"	<div><div>It is just a compact way of representing a Bayesian network.</div></div>
"Given the DAG below, which pairs of variables are d-separated given {C,E}?<br><img src=""paste-84ba04b0bc5878e8404da6596feb9d7fac2e0945.jpg"">"	1) A,D<br>2) B,D
"Given the DAG below, which pairs of variables are d-separated given {E}?<br><img src=""paste-84ba04b0bc5878e8404da6596feb9d7fac2e0945.jpg"">"	None of them, mwhehehe.
"Given the DAG below, which pairs of variables are d-separated given {C}?<br><img src=""paste-84ba04b0bc5878e8404da6596feb9d7fac2e0945.jpg"">"	1) A,D<br>2) B,D<br>3) D,E
"Given the DAG below, which pairs of variables are d-separated given {C}?<br><img src=""paste-84ba04b0bc5878e8404da6596feb9d7fac2e0945.jpg"">"	1) A,B
<div><div>What assumption is made when using a naive Bayes model?</div></div>	<div><div>The features are independent of each other conditional on the class variable (thing we're trying to predict or classify). Example:</div></div><div><br></div><div>Lets consider the DAG:</div><div><br></div><div>StressLvl &lt;- PassML -&gt; HoursStudiedPointlessly</div><div><br></div><div>Where PassML is our class variable. Once we know if we passed&nbsp;how many hours we studied doesn’t give extra info about our stress level — the class variable “explains away” the correlation. The fact we passed already explains the combination of stress and study. If we didn't know, you might reason: “If they studied a lot, maybe they are more stressed because they are working hard.”</div><div><br></div><div>Nice thing about this DAG representation is that all pairs of variables (not including the class var) are d-seperated by the class variable. Ie: once you know the class variable, all variables are independent of each other.</div><div><br></div><div>For a DAG to represet a Naive Bayes, the class variable must be the only parent to all variables and the variables must have no edges between them.</div><div><br></div>
<div><div>When a Bayesian network is used to fully represent a machine learning model what does the BN represent?</div></div>	<div><div>The joint distribution of all random variables involved including: all parameters,&nbsp;all observed variables (data),&nbsp;and any latent variables.&nbsp;</div></div>
<div><div>What is the advantage of using hierarchical Bayesian linear regression?</div></div>	<div><div>Data collected from a particular location influence the posterior distribution of parameters for other locations but only to a degree.&nbsp;We thus have a balance between location-specific learning and using data for all locations.</div></div>
Which term is particularly expensive to compute in Bayesian inference?<br>Hint: Think about the MCMC lab. What part was computationally hard to find that meant we couldn't sample from the distribution?	"<div><div>Evidence (or normalization).</div></div><div><br></div><div>\(\underbrace{P(\theta \mid \mathcal{D})}_{\text{Posterior}} 
= 
\frac{
\underbrace{P(\mathcal{D} \mid \theta)}_{\text{Likelihood}} 
\underbrace{P(\theta)}_{\text{Prior}}
}{
\underbrace{P(\mathcal{D})}_{\text{Evidence (Normalizing Constant)}}
}\)<br><br></div><div>\(P(\mathcal{D}) = \int P(\mathcal{D} \mid \theta) P(\theta) \, d\theta\)<br></div><div><br></div><div><div>Evidence is a number: it’s the integral over all parameter space that ensures the posterior is normalized. (all probs, or area underneath, add to 1 like a proper dist).</div> <div>Samples (from e.g., MCMC) are specific draws of \(θ\) from the posterior. They are used to approximate the posterior without computing the evidence exactly.</div> <div>In fact, one of the main reasons we use sampling methods like MCMC is to avoid computing the evidence explicitly, because it can be very expensive in high-dimensional problems.</div></div>"
<div><div>Given this likelihood function&nbsp;p(a|b)&nbsp;what is the respective Bayesian theorem?</div></div>	<div><div>p(b|a)=p(a|b)p(b)/p(a)</div></div><div><br></div><div>As p(a|b) is the likelihood so p(b|a) is the posterior.</div>
<div><div>Why is Bayesian inference good?</div></div>	<div><div>Explicitly&nbsp;encodes&nbsp;prior knowledge and uncertainty.</div></div><div><br></div><div>Idek after some research so... yeah.</div>
<div><div>Why do we throw away initial samples from an MCMC run?</div></div>	<div><div>Because these sampled values are sampled from distributions which are unlikely to be close to the target distribution.</div></div><div><br></div><div>We often just guess at a starting point as the algorithm converges closer to the target dist.</div>
<div><div>When using Metropolis-Hastings what do we do with the sampled value generated from the proposal distribution?</div></div>	<div><div>We use its value to compute an acceptance probability and use this probability to decide whether it should be the next state.</div></div>
<div><div>What is an invariant distribution of a Markov chain?</div></div>	<div><div>If p(z|z')&nbsp;is the transition probability for a Markov chain then p' is invariant for it&nbsp;if p'(z') = p(z'|z)p'(z).</div></div><div><br></div><div>LHS: Probability of the next state being z'</div><div>RHS: Probability of being in z and moving to z'</div>
<div><div>Why is it sensible to do several independent runs of MCMC?</div></div>	<div><div>To compare results from the different runs.&nbsp;If they produce different significantly different estimates then something is wrong.<br><br>This might happen when the target distribution has multiple peaks seperated by low prob regions (if you started from different points in your runs) or if your chains havent run long enough they might not have converged.</div></div>
What is the ReLU formula?	\[f(x) = max(0,x)\]<br>
What is a Classification Tree?	A classification tree is a type of decision tree used in supervised learning where the goal is to assign an input to one of several category labels (classes). When traversing the binary decision tree, the final decision is made by a leaf node. For classification tasks, this leaf node is responsible for assigning a decision, which may be a class label or a probability distribution over class labels.
What is a Regression Tree?	A regression tree is a type of decision tree used in supervised learning where the goal is to predict a numerical output (a scalar value). The final prediction is assigned by a leaf node, which is responsible for giving a scalar value for regression tasks.
Are decision trees parametric or non-parametric?	Trees are a nonparametric method. (more data = more parameters)
Describe the standard greedy algorithm (CART) for learning trees, including the final pruning stage.	1. Start from the root node.<br>2. Run an exhaustive search over every possible input variable and threshold for a new split.<br>3. For each possible split, compute the average of the target variable for the proposed leaves and then calculate the error (or cost) that would result if the tree construction stopped there.<br>4. Choose the variable and threshold that minimise the error.<br>5. Repeat this process until a stopping condition is met, typically until there are only n data points associated with each leaf node. The error minimized depends on the task: sum-of-squares for regression, and cross-entropy or Gini impurity for classification. <br>The Final Pruning Stage: After the full tree is grown, the final step involves pruning back the tree. Pruning balances the residual training-set error against the model complexity. Pruning removes branches that do not reduce the error by more than a small tolerance value (ϵ). This is often achieved bottom-up by combining branches (merging leaves).
What is the main flaw of decision trees?	If the optimal decision boundary for the classification or regression task is not aligned with the axes of the input variables, the tree learning process may require a lot of splits to approximate the boundary effectively.
What is a kernel function?	A function&nbsp;\(k(x,x')\)&nbsp;that intrinsically measures the degree of similarity between two raw data vectors,&nbsp;\(x\)&nbsp;and&nbsp;\(x'\).
What is the kernel trick?	A computational shortcut that involves evaluating the kernel function directly without first needing to explicitly compute the feature vectors&nbsp;\(ϕ(x)\)&nbsp;and&nbsp;\(ϕ(x')\)&nbsp;and then calculating their scalar product.
What properties must a custom kernel function have?	It must be symmetric.&nbsp;It is formally equivalent to the scalar product of the corresponding feature vectors in a transformed feature space:&nbsp;\(k(x,x′)=ϕ(x)^T ϕ(x′)\). The resulting Gram matrix must be symmetric and positive semidefinite.
What is the Gram Matrix?	A matrix defined as&nbsp;\(K=ΦΦ^T\), where&nbsp;\(Φ\)&nbsp;is the design matrix. The entry \(K_{nm}\)​ represents the kernel similarity&nbsp;\(k(x_n,x_m)\)&nbsp;between the n-th and m-th data point in the training set.&nbsp;The Gram matrix contains all the pairwise similarities computed by the kernel function across the entire training dataset.
What is the margin in linear classification?	The distance from the separating hyperplane to the closest training datapoint.
Why are SVMs a type of&nbsp;maximum margin classifiers?	They choose the hyperplane that actively maximises this distance to provide a robust separation between classes.
What are support vectors?	These are considered the training data points 'that matter' for defining the decision boundary. Mathematically, they are defined as those data points&nbsp;whose corresponding dual parameter is non-zero. Only these vectors are needed to make predictions for new data points. They are the data points that lie closest to the separating hyperplane.
What is the soft margin approach?	It relaxes the strict linear separability constraint to allow for a wider margin, even if it means some training points fall inside the margin or are misclassified. It is often used to prevent overfitting.
What are slack variables?	Variables introduced in the soft margin optimisation problem to quantify the degree of violation of the margin constraint by each training data point.
What is the&nbsp;role of the regularisation parameter in soft margins?	The regularisation parameter,&nbsp;\(C\), appears in the soft margin primal optimisation objective:&nbsp;\(\min \frac{1}{2} \mathbf{w}^{\top}\mathbf{w} + C \sum_{n=1}^{N} \xi_n\).&nbsp;The value of \(C\)&nbsp;dictates the trade-off between two competing goals: maximising the margin and **minimising the training error. A large value of $C$ means that misclassification is heavily penalised.
How can SVMs be extended to deal with having more than two classes?	<div>One-vs-One (OVO): Trains \(k(k-1)/2\)&nbsp;classifiers, one for each pair of classes. Predicts by majority vote.<br></div><div>One-vs-Rest (OVR): Trains&nbsp;\(k\)&nbsp;classifiers, one for each class against all remaining classes.</div>
What is the RBF kernel?	<div>The Radial Basis Function kernel is defined as&nbsp;\(k(\mathbf{x}, \mathbf{x}') = \exp(-\gamma\|\mathbf{x}- \mathbf{x}'\|^2)\).<br></div><div>Feature Space: Maps data to an infinite-dimensional space (making the model non-parametric).</div><div>Parameter&nbsp;\(\gamma\): Controls the influence radius of support vectors.</div><div>Low&nbsp;\(\gamma\): Far influence (smoother decision boundary).</div><div>High&nbsp;\(\gamma\): Close influence (can lead to overfitting).</div>
How do kernel-based methods make predictions?	<div>Prediction Formula: They predict a new point&nbsp;\(x\)&nbsp;by computing a weighted sum of its similarities (using the kernel&nbsp;\(k\)) to all training points&nbsp;\(x_n\)​:&nbsp;</div><div>The Problem: This seems to require storing the entire training set to make any new predictions.<br></div>\(y(\mathbf{x}) = \sum_{n=1}^{N} a_n k(\mathbf{x}_n, \mathbf{x}) + b\)<div>SVM's Solution (Sparsity): SVMs are efficient because most of the weights&nbsp;\(a_n\)&nbsp;become zero. Only the non-zero points, the Support Vectors (SVs), are needed for prediction.<br></div>
What graphical element is fundamental to defining the structure of a Bayesian network?	A Directed Acyclic Graph (DAG). This is a graph structure containing nodes (random variables) and directed edges (arrows). It must be acyclic, meaning there are no directed paths that return to the starting node.
Define conditional independence between random variables,&nbsp;\(x\)&nbsp;and&nbsp;\(y\),&nbsp;given a set,&nbsp;\(S\),&nbsp;of random variables.	\(x\)&nbsp;is independent of&nbsp;\(y\)&nbsp;conditional on&nbsp;\(S\)&nbsp;if and only if&nbsp;\(P(x, y |S) = P(x |S)P(y |S)\). This is equivalent to&nbsp;\(P(x |S) = P(x |y ,S)\). Intuitively, once the value of&nbsp;\(S\)&nbsp;is known, knowing&nbsp;\(y\)&nbsp;provides no additional information about&nbsp;\(x\).
What is a Bayesian network (BN)?	A type of Probabilistic Graphical Model (PGM). It provides a graphical representation of how a joint distribution factorises by explicitly depicting the conditional independence relations present within that distribution.
In a Bayesian network, if there is an arrow from node A to node B, what is A called relative to B?	A is a parent*of B. The set of parents of a node&nbsp;\(x_k\)&nbsp;is typically denoted \(\text{pa}_k\).
When applying d-separation, which nodes are considered descendants of a collider?	A descendant is any node reachable from the collider by following the direction of the arrows.&nbsp;Descendants of a collider are important because if they are included in the conditioning set, they can unblock the path through the collider.
What defines a path in a Bayesian network?	A path is a sequence of connected nodes in the DAG, regardless of the arrow direction along the sequence.
What is a collider on a path in a Bayesian network?	A node&nbsp;\(C\)&nbsp;is a collider on a path if the path enters&nbsp;\(C\)&nbsp;via two incoming arrows, meaning both arrows on that specific path point into it (e.g.,&nbsp;\(A \rightarrow C \leftarrow B\)). Conditioning on a collider or one of its descendants unblocks the path,.
State the two conditions under which a path is considered blocked by a set of conditioning nodes&nbsp;\(S\).	A path is blocked by&nbsp;\(S\)&nbsp;if at least one of the following is true:<br>1. There is a collider on the path that is not in&nbsp;\(S\)&nbsp;and none of its descendants are in&nbsp;\(S\). As, if the collider or a descendant is in&nbsp;\(S\), the path is unblocked.<br>2. There is a non-collider on the path that is in&nbsp;\(S\).<br>If all paths between two nodes are blocked by&nbsp;\(S\), they are d-separated.
State the general factorisation of the joint probability distribution&nbsp;\(p(\mathbf{x})\)&nbsp;defined by the structure of a given Bayesian network.	The joint distribution \(p(\mathbf{x})\)&nbsp;over&nbsp;\(K\)&nbsp;variables \(x_1, \ldots, x_K\)&nbsp;factorises as:<br>\(p(\mathbf{x}) = \prod_{k=1}^{K} p(x_k | \text{pa}_k) \text{}\)<br>This formula holds because the DAG structure implies that the conditional distribution of any variable \(x_k\)&nbsp;depends only on its immediate parents (\(\text{pa}_k\)).
How to use d-separation to check for conditional independence relations in a Bayesian network?	Two nodes are deemed conditionally independent given a set if and only if they are d-separated by that set.
What is the Likelihood?	The Likelihood, denoted \(P(D|\theta)\), is the conditional probability distribution of observing the data&nbsp;\(D\)&nbsp;given specific values for the model parameters&nbsp;\(\theta\).
What is the Posterior distribution?	The Posterior distribution, denoted \(P(\theta|D=d)\), is the distribution representing the updated beliefs about the model parameters&nbsp;\(\theta\)&nbsp;after the observed data&nbsp;\(d\)&nbsp;has been taken into account. Computing this distribution is the conceptual goal of the Bayesian approach, typically achieved using Bayes’ theorem: \(P(\theta|D=d) \propto P(\theta)P(D=d|\theta)\).
In the Bayesian approach, how are parameters, data, and unobserved (latent) variables represented?	In the Bayesian approach, the parameters, the data , and any unobserved (latent) variables are all represented as random variables within a joint probability distribution. The unknown quantities (parameters and latent variables) are specifically treated as unobserved random variables, while the known quantities (the data) are treated as observed random variables.
